<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>index</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="machine-learning-and-predictive-analytics-in-health-care" class="level1">
<h1>MACHINE LEARNING AND PREDICTIVE ANALYTICS IN HEALTH CARE</h1>
<section id="developing-models-to-predict-outcomes-in-healthcare" class="level3">
<h3 class="anchored" data-anchor-id="developing-models-to-predict-outcomes-in-healthcare">Developing Models to Predict Outcomes in Healthcare</h3>
</section>
<section id="table-of-contents" class="level2">
<h2 class="anchored" data-anchor-id="table-of-contents">Table of Contents</h2>
<ol type="1">
<li><a href="#introduction">Introduction</a>
<ul>
<li><a href="#Problem-Defination">Problem Defination</a><br>
</li>
<li><a href="#context-and-background">Context and Background</a><br>
</li>
<li><a href="#objectives-and-goals">Objectives and Goals</a><br>
</li>
<li><a href="#summary-of-approach">Summary of Approach</a><br>
</li>
</ul></li>
<li><a href="#methods">Methods</a>
<ul>
<li><a href="#data-acquisition-and-sources">Data Acquisition and Sources</a><br>
</li>
<li><a href="#mathematical-and-statistical-models">Mathematical and Statistical Models</a><br>
</li>
<li><a href="#experimental-design-and-analytical-procedures">Experimental Design and Analytical Procedures</a><br>
</li>
<li><a href="#software-and-tools">Software and Tools</a><br>
</li>
<li><a href="#ethical-considerations">Ethical Considerations</a></li>
</ul></li>
<li><a href="#results-and-discussion">Results and Discussion</a><br>
</li>
<li><a href="#conclusion">Conclusion</a><br>
</li>
<li><a href="#references">References</a></li>
</ol>
<hr>
</section>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">INTRODUCTION</h2>
<section id="problem-defination" class="level3">
<h3 class="anchored" data-anchor-id="problem-defination">PROBLEM DEFINATION</h3>
<p>Machine learning enables healthcare prediction because healthcare institutions have better access to patient data that lets them implement earlier interventions employing individualized therapeutic approaches. The goal of this work utilizes a patient records database to create diagnostic prediction models that will assist clinicians in both better diagnosis and healthcare decision processes.</p>
</section>
<section id="context-and-background" class="level3">
<h3 class="anchored" data-anchor-id="context-and-background">CONTEXT AND BACKGROUND</h3>
<p>Healthcare predictive analysis performs pattern identification through statistical and machine learning operations on patient information. Logistic regression functions as one of the traditional methods utilized for binary classification since a long time. The predictive power of Support Vector Machines (SVM) and XGBoost increases because they can identify sophisticated patterns present in the data. The assessment of these models in actual healthcare applications becomes possible through analysis of the CSV dataset which contains demographic details and symptom information along with diagnosed diseases.</p>
</section>
<section id="objectives-and-goals" class="level3">
<h3 class="anchored" data-anchor-id="objectives-and-goals">OBJECTIVES AND GOALS</h3>
<p>The main objectives of our capstone project focus on analytics which we detail in the three points listed below.</p>
<ul>
<li>Create multiple machine learning models alongside their evaluation to identify diseases.</li>
<li>A comparison of the disease classification outcomes between logistic regression, SVM and XGBoost must be evaluated.</li>
<li>The solution provides data assessment techniques to discover diseases early and enhance medical outcomes.</li>
<li>The team conducts exploratory data analysis which includes prediction trend analysis by using visualizations.</li>
<li>The system generates multiple plots containing confusion matrices together with ROC curves and feature importance charts.</li>
<li>Using identified patient characteristics which affect disease predictions will help healthcare practitioners make better decisions.</li>
<li>Evaluating the model’s resilience requires using hyperparameter adjustment along with cross-validation.</li>
</ul>
</section>
<section id="summary-of-approach" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-approach">SUMMARY OF APPROACH</h3>
<p>The initial stage of data analysis with exploratory data analysis (EDA) involves identifying missing data as well as studying distribution patterns and significant disease outcome influencing features. Good quality inputs into the machine learning model are achieved through preprocessing steps which involve data cleaning followed by encoding categorical values and performing feature scaling. Patient data classification uses three predictive models including Logistic Regression as well as Support Vector Machine (SVM) and XGBoost to perform disease outcomes prediction. The training procedure starts with data splitting after which performance optimization uses hyperparameter tuning followed by applying cross-validation methods. Model assessment requires metrics based on accuracy measurements supported by precision recall and F1-score values as well as AUC-ROC curves.</p>
<p>This interpretation improves by using three specific visualization tools consisting of confusion matrices and feature importance plots and ROC curves. Medical diagnosis platforms for healthcare administrations prove more effective due to disease prediction indicators according to this research study.</p>
<section id="logistic-regression" class="level4">
<h4 class="anchored" data-anchor-id="logistic-regression">LOGISTIC REGRESSION</h4>
<p>Logistic Regression functions as a statistical method dedicated to binary classification by determining disease occurrence probabilities.</p>
</section>
<section id="support-vector-machine" class="level4">
<h4 class="anchored" data-anchor-id="support-vector-machine">SUPPORT VECTOR MACHINE</h4>
<p>The classification technology Support Vector Machines (SVM) detects the best dividing hyperplane which separates diverse disease classes.</p>
</section>
<section id="xgboost" class="level4">
<h4 class="anchored" data-anchor-id="xgboost">XGBOOST</h4>
<p>XGBoost functions as an algorithm which uses gradient boosting to construct precise predictions from several weak prediction models.</p>
</section>
</section>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">METHODOLOGY</h2>
<section id="data-and-sources" class="level3">
<h3 class="anchored" data-anchor-id="data-and-sources">DATA AND SOURCES</h3>
<p>The “Disease Symptoms and Patient Profile Dataset” from Kaggle served as the origin of the analyzed data. The data includes population information including age and sex data as well as patient-reported symptoms and tested disease results. The data collection process used healthcare records in combination with patient symptom assessments which underwent anonymous data processing. The preprocessing process includes three core procedures to replace missing data elements followed by identifying numeric representations for classification data and applying normalization techniques for numerical values for achieving better consistency and predictive success. The “Disease Symptoms and Patient Profile Dataset” includes patient attributes that combine age with gender and symptoms as well as diagnosed diseases. The processing stage performs various operations which include null value treatment followed by group data encoding and numerical value normalization.</p>
</section>
<section id="mathematical-models" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-models">MATHEMATICAL MODELS</h3>
<section id="logistic-regression-1" class="level4">
<h4 class="anchored" data-anchor-id="logistic-regression-1">LOGISTIC REGRESSION</h4>
<p>The linear logistic regression model serves as a method for two-category classification. Using the sigmoid function logistic regression identifies the probability that a disease will appear.</p>
<p><span class="math display">\[
P(y=1|X) = \frac{1}{1+e^{- (\beta_0 + \beta_1 X_1 + \dots + \beta_n X_n)} }
\]</span></p>
</section>
<section id="support-vector-machine-1" class="level4">
<h4 class="anchored" data-anchor-id="support-vector-machine-1">SUPPORT VECTOR MACHINE</h4>
<p>The SVM technique generates a boundary hyperplane which splits data classes inside multi-dimensional spaces. The decision boundary emerges from maximizing the separation distance between data points:</p>
<p><span class="math display">\[
f(X) = w^T X + b
\]</span></p>
<p>w is the weight vector and b is the bias term.</p>
</section>
<section id="xgboost-1" class="level4">
<h4 class="anchored" data-anchor-id="xgboost-1">XGBOOST</h4>
<p>XGBOOST is the ensemble learning technique that builds multiple decision trees.</p>
<pre class="math"><code>L(\theta) = \sum_{i=1}^{n} l(y_i, \hat{y}_i) + \sum_{k} \Omega(f_k)</code></pre>
<p>l is the loss function and omega is the regularization term.</p>
</section>
</section>
<section id="experimental-design-and-analytical-procedures" class="level3">
<h3 class="anchored" data-anchor-id="experimental-design-and-analytical-procedures">EXPERIMENTAL DESIGN AND ANALYTICAL PROCEDURES</h3>
<p>The analytical steps inlcude the following;</p>
<ol type="1">
<li><p>DATA PREPROCESSING: The preprocessing stage includes addressing data gaps while converting categorical groups into numerical forms (symptoms) and age normalization for numerical attributes.</p></li>
<li><p>FEATURE SELECTION: Analysis of disease prediction features requires techniques that use correlation analysis and feature importance ranking methods.</p></li>
<li><p>MODEL TRAINING: The baseline model of Logistic Regression received training as a benchmark for describing model performance.Support Vector Machine uses its algorithm to discover the hyperplane which provides maximum separation between different classes. XGBoost operates through boosting methodology to combine various weak learners that improve classification outcomes.</p></li>
<li><p>EVALUATION: The successful measurement of model accuracy requires determining the ratio of accurate predictions and precision assesses the true positive count against all detected positives. Model performance regarding its ability to detect positive cases is measured by recall and its classification capabilities are evaluated through AUC-ROC measurements.</p></li>
<li><p>INTERPRETATION: The evaluation of models through performance metrics along with analysis of their feature importance allows practitioners to identify which patient symptoms and characteristics best indicate particular diseases.<br>
</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/WhatsApp%20Image%202025-02-24%20at%208.10.32%20PM.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</section>
<section id="software-tools" class="level3">
<h3 class="anchored" data-anchor-id="software-tools">SOFTWARE TOOLS</h3>
<section id="programming-languages" class="level4">
<h4 class="anchored" data-anchor-id="programming-languages">PROGRAMMING LANGUAGES</h4>
<ol type="1">
<li>PYTHON : This is the primary language that is used in this capstone project for preprocessing the data, model training and the evaluation.</li>
<li>MACHINE LEARNING : With the help of machine learning, this project could be able to predict the outcomes and do predictive analysis with data driven decisions.</li>
</ol>
</section>
<section id="environment" class="level4">
<h4 class="anchored" data-anchor-id="environment">ENVIRONMENT</h4>
<p>GOOGLE COLAB : I have used google colab for the cloud-based execution of my code, I believe it has exceptional and enhanced computational resources.</p>
</section>
<section id="libraries-and-models" class="level4">
<h4 class="anchored" data-anchor-id="libraries-and-models">LIBRARIES AND MODELS</h4>
<ol type="1">
<li><p>PANDAS : Pandas are used for data manipulation and pre processing the datasets</p></li>
<li><p>NUMPY : It is used for numerical computations and data handling</p></li>
<li><p>SCIKIT-LEARN : scikit learn is used for implementing the models in the code such as SVM, Logistic and random forest.</p></li>
<li><p>XGBOOST : To perform a better feature analysis and handling.</p></li>
<li><p>MATPLOTLIB AND SEABORN : We use matplotlib and seaborn to mke visualization of the analysis for the better understanding of the data, and as well for exploratory analysis.</p></li>
</ol>
</section>
</section>
<section id="ethical-considerations" class="level3">
<h3 class="anchored" data-anchor-id="ethical-considerations">ETHICAL CONSIDERATIONS</h3>
<p>The dataset contains anonymized patient data to protect privacy. Data management needs strict ethical treatment alongside equal model prediction distribution and unbiased healthcare choices must be ensured.</p>
</section>
</section>
<section id="expreimental-procedure" class="level2">
<h2 class="anchored" data-anchor-id="expreimental-procedure">EXPREIMENTAL PROCEDURE</h2>
<p>This project includes various steps to be considered as follows;</p>
<p><strong>INSTALL LIBRARIES</strong></p>
<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns</code></pre>
<p><strong>IMPORT SCIKIT LEARN TOOLS</strong></p>
<pre><code>from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report</code></pre>
<p><strong>READING THE DATA</strong></p>
<pre><code>file_path = "/content/Disease_symptom_and_patient_profile_dataset.csv"
df = pd.read_csv(file_path)</code></pre>
<p><strong>DATA CLEANING AND TRIMMING</strong></p>
<pre><code>label_encoders = {}
for col in df.columns:
    if df[col].dtype == 'object':
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        label_encoders[col] = le</code></pre>
<p><strong>TRAIN TEST SPLIT TRAINING AND MODEL</strong></p>
<pre><code>X = df.drop(columns=['Outcome Variable'])
y = df['Outcome Variable']

# Split dataset into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss')
}

# Train and evaluate each model
for name, model in models.items():
    print(f"\nModel: {name}")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    conf_matrix = confusion_matrix(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")
    print("Confusion Matrix:\n", conf_matrix)
    print("Classification Report:\n", class_report)</code></pre>
<section id="logistic-regression-2" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-2">LOGISTIC REGRESSION</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/WhatsApp%20Image%202025-03-10%20at%209.23.01%20PM.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500"></p>
</figure>
</div>
</section>
<section id="xgboost-2" class="level3">
<h3 class="anchored" data-anchor-id="xgboost-2">XGBOOST</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/WhatsApp%20Image%202025-03-10%20at%209.23.02%20PM.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500"></p>
</figure>
</div>
<p><strong>DATA VISUALISATION</strong></p>
<p>### CONFUSION MATRIX</p>
<pre><code>    plt.figure(figsize=(6, 4))
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoders['Outcome Variable'].classes_, yticklabels=label_encoders['Outcome Variable'].classes_)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'{name} - Confusion Matrix')
    plt.show()</code></pre>
<section id="losgistic-regression" class="level4">
<h4 class="anchored" data-anchor-id="losgistic-regression">LOSGISTIC REGRESSION</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/WhatsApp%20Image%202025-03-10%20at%209.23.02%20PM%20(1).jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500"></p>
</figure>
</div>
</section>
<section id="xgboost-3" class="level4">
<h4 class="anchored" data-anchor-id="xgboost-3">XGBOOST</h4>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/WhatsApp%20Image%202025-03-10%20at%209.23.03%20PM.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="500"></p>
</figure>
</div>
<pre><code>sns.pairplot(df, hue='Outcome Variable')
plt.suptitle('Pairwise Feature Relationships', y=1.02)
plt.show()

plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Feature Correlation Heatmap')
plt.show()</code></pre>
</section>
<section id="feature-co-relation-heatmap" class="level4">
<h4 class="anchored" data-anchor-id="feature-co-relation-heatmap">FEATURE CO-RELATION HEATMAP</h4>
<p><img src="images/WhatsApp%20Image%202025-03-10%20at%209.23.06%20PM.jpeg" class="img-fluid" width="600"></p>
</section>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">RESULTS</h2>
<p>Our research related to Logistic Regression and XGBoost models yields these results in this section. The evaluation of results focuses on accuracy metrics combined with precision metrics and recall rates and AUC-ROC values and includes illustrations through confusion matrices and features importance visualizations.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Metric</th>
<th>Logistic Regression</th>
<th>XGBoost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>0.76</td>
<td>0.89</td>
</tr>
<tr class="even">
<td>Precision</td>
<td>0.75</td>
<td>0.87</td>
</tr>
<tr class="odd">
<td>Recall</td>
<td>0.73</td>
<td>0.88</td>
</tr>
<tr class="even">
<td>AUC Score</td>
<td>0.80</td>
<td>0.93</td>
</tr>
</tbody>
</table>
<p><strong>model comparison table</strong></p>
<p>XGBoost produces superior performance than Logistic Regression through all evaluated evaluation metrics according to these findings. XGBoost proves more suitable for healthcare disease classification tasks because its higher precision and recall values show better ability to detect true positives and decrease false negatives.</p>
<section id="confusion-matrix" class="level3">
<h3 class="anchored" data-anchor-id="confusion-matrix">CONFUSION MATRIX</h3>
<p><strong>LOGISTIC REGRESSION</strong> The confusion matrix assessment for Logistic Regression displays moderate outcomes due to multiple both true and false classifications. Correct classification stands essential for clinical decisions thus this performance may impact medical decision-making.</p>
<p><strong>XGBOOST</strong> Improvements from XGBoost model go beyond other approaches because it produces less misclassified instances. Better management of non-continuous relationships along with handling of multi-dimensional information leads XGBoost to provide superior predictive accuracy for new data points.</p>
</section>
<section id="feature-importance-analysis" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-analysis">FEATURE IMPORTANCE ANALYSIS</h3>
<p>The decision-making pattern of both Logistic Regression and XGBoost differs through their algorithms despite providing feature importance insights.</p>
<p>Low importance in feature analysis of Logistic Regression originates from the large magnitude of weight values. The model establishes that feature variables create a direct connection with the calculated logarithmic probabilities of outcome predictions. Logistic Regression evaluated Patient Age and Gender and numerical Symptom Indicator features to be most influential among all features in our analysis. The interpretability strength of this method reaches its limit when responding to intricate nonlinear patterns within the data set.</p>
<p>Gradient boosting decision trees in XGBoost detect complex patterns and feature interactions through its unique implementation. XGBoost extract feature importance ratings by counting how often each variable splits the records when building all trees in the model. Our model shows Symptom Severity Score and Patient Age together with Chronic Conditions Indicator and Gender as the key variables that influence predictions.</p>
<p>The XGBoost methodology showed better feature ranking precision than Logistic Regression and produced discoveries about feature interactions the latter could not generate. XGBoost delivered better predictive results because of its design principle in this evaluation.</p>
</section>
<section id="visualisation" class="level3">
<h3 class="anchored" data-anchor-id="visualisation">VISUALISATION</h3>
<p><strong>Correlation Heatmap</strong></p>
<p>The heatmap confirms strong positive and negative correlations that exist between symptoms and the outcome variable thus validating the accuracy of both feature selection and model interpretation.</p>
<p><strong>Pairwise Feature Relationships</strong></p>
<p>By visualizing variable interactions with each other and with the outcome variable the pairplot helps establish clearer understanding of class discriminability.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">CONCLUSION</h2>
<p>XGBoost demonstrates significant potential for healthcare diagnostic accuracy enhancement according to the research outcomes. This model identifies crucial characteristics and symptoms from patients to deliver diagnosis-related information that supports medical decisions.</p>
<p>The integration of XGBoost in clinical decision support systems becomes favorable because of its exceptional performance metrics along with its clear interpretability. Disease diagnosis at an early correct stage both benefits patient health outcomes and enhances healthcare system operational efficiency.</p>
<p>Data preparation requires proper robust methods according to this research study. The way missing data is handled together with variable encoding and normalization techniques dramatically affects the performance levels of overall models. Research into this project illustrates how joint healthcare expertise and data scientific methods fuel innovative changes in healthcare.</p>
<p>Future development of this model should consist of implementing extended patient tracking through time while linking in-time symptom assessment along with multi-source data validation to boost prediction reliability across different datasets. The future development of these models should target implementation within operational health informatics systems that support interactive use by healthcare professionals.</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">REFERENCES</h2>
<ol type="1">
<li>Kaggle Dataset Citation : Kaggle. (n.d.). Disease Symptoms and Patient Profile Dataset. Retrieved from: https://www.kaggle.com/Disease_symptom_and_patient_profile_dataset.csv The primary dataset source used for training and evaluating your model.</li>
<li>Jiang et al.&nbsp;(2017). “Artificial intelligence in healthcare: past, present and future.” BMJ</li>
<li>Obermeyer et al.&nbsp;(2016). “Predicting patient risk using machine learning: a retrospective cohort study.” The Lancet</li>
</ol>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>